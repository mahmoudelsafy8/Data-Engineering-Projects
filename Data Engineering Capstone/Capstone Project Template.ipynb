{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project\n",
    "### Immigration Study in US by Demographics and Temperatures\n",
    "\n",
    "#### Project Summary\n",
    "I'll work with four datasets to complete the project. The main dataset will include data on immigration to the United States, and supplementary datasets will include data on airport codes, U.S. city demographics, and temperature data. You're also welcome to enrich the project with additional data.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "import datetime\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "from pyspark.sql.functions import year, month, dayofmonth, weekofyear, date_format, dayofweek,monotonically_increasing_id,udf,col,when,count,avg,isnan\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData, HiveContext\n",
    "from pyspark.sql.types import DoubleType, DateType, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "##### Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use?\n",
    "\n",
    "In this Project I will gather the data from four sources, and Loads this data into staging dataframes, and  Cleans the raw data, then write it to parquet files and perform an ETL process using Spark cluster, then add data into Fact & Dimension tables to form schema.\n",
    "The schema can be used perform data analytices, correlation and ad-hoc reporting in an effective.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "-  **i94 Immigration sample Data:** \n",
    "\n",
    "Sample data of immigration records from th US National Tourism and Trade Office, this data source will serve as the Fact table in the schema, and this data comes from: https://www.trade.gov/national-travel-and-tourism-office\n",
    "\n",
    "- **World Temperature Data:**\n",
    "\n",
    "This datset contains temperature data in different cities from the 1750 to 2013, this data comes from : https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "\n",
    "- **U.S. City Demographic Data:**\n",
    "\n",
    "This dataset includes information on the population of all US cities such as race, household size and gender, this data comes from: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "\n",
    "- **Airport Codes Table:**\n",
    "\n",
    "This dataset table contains the airport codes for the corresponding cities, this data comes from: https://datahub.io/core/airport-codes#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read immigration data\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_df = spark.read.format('com.github.saurfang.sas.spark').load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN    None   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U     None   1979.0  10282016   None   None   \n",
       "1      NaN   ...           Y     None   1991.0       D/S      M   None   \n",
       "2  20691.0   ...        None        M   1961.0  09302016      M   None   \n",
       "3  20567.0   ...        None        M   1988.0  09302016   None   None   \n",
       "4  20567.0   ...        None        M   2012.0  09302016   None   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0    None  1.897628e+09   None       B2  \n",
       "1    None  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the head of data\n",
    "immigration_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>AR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state code\n",
       "0     Alabama   AL\n",
       "1      Alaska   AK\n",
       "2     Arizona   AZ\n",
       "3    Arkansas   AR\n",
       "4  California   CA"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and Review states data \n",
    "fname = ('us_states.csv')\n",
    "us_states = spark.read.csv(fname,header=True,inferSchema=True)\n",
    "us_states.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read temperature data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperature_df = spark.read.csv(fname, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0 1743-11-01               6.068                          1.737  Århus   \n",
       "1 1743-12-01                 NaN                            NaN  Århus   \n",
       "2 1744-01-01                 NaN                            NaN  Århus   \n",
       "3 1744-02-01                 NaN                            NaN  Århus   \n",
       "4 1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the head of data\n",
    "temperature_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### US Cities Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read demographics data by pandas\n",
    "df_demographics = pd.read_csv('us-cities-demographics.csv',sep=';')\n",
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>avg_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  median_age  male_population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   female_population  total_population  number_veterans  foreign_born  \\\n",
       "0            41862.0             82463           1562.0       30908.0   \n",
       "1            49500.0             93629           4147.0       32935.0   \n",
       "2            46799.0             84839           4819.0        8229.0   \n",
       "3            87105.0            175232           5821.0       33878.0   \n",
       "4           143873.0            281913           5829.0       86253.0   \n",
       "\n",
       "   avg_household_size state_code                       Race  Count  \n",
       "0                2.60         MD         Hispanic or Latino  25924  \n",
       "1                2.39         MA                      White  58723  \n",
       "2                2.58         AL                      Asian   4759  \n",
       "3                3.18         CA  Black or African-American  24437  \n",
       "4                2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the columns names \n",
    "df_demographics = df_demographics.rename(columns={'Median Age':'median_age','Male Population':'male_population',\n",
    "                                                 'Female Population':'female_population','Total Population':'total_population',\n",
    "                                                 'Number of Veterans':'number_veterans','Foreign-born':'foreign_born',\n",
    "                                                 'Average Household Size':'avg_household_size','State Code':'state_code'})\n",
    "# review the head of data to check the change \n",
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the change to the dataset \n",
    "df_demographics.to_csv('us-cities-demographics1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read US cities Demographics data\n",
    "fname = 'us-cities-demographics1.csv'\n",
    "demographics_df = spark.read.csv(fname,inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>avg_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  median_age  male_population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   female_population  total_population  number_veterans  foreign_born  \\\n",
       "0            41862.0             82463           1562.0       30908.0   \n",
       "1            49500.0             93629           4147.0       32935.0   \n",
       "2            46799.0             84839           4819.0        8229.0   \n",
       "3            87105.0            175232           5821.0       33878.0   \n",
       "4           143873.0            281913           5829.0       86253.0   \n",
       "\n",
       "   avg_household_size state_code                       Race  Count  \n",
       "0                2.60         MD         Hispanic or Latino  25924  \n",
       "1                2.39         MA                      White  58723  \n",
       "2                2.58         AL                      Asian   4759  \n",
       "3                3.18         CA  Black or African-American  24437  \n",
       "4                2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review the head of data\n",
    "demographics_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read airport data\n",
    "fname = 'airport-codes_csv.csv'\n",
    "airport_df = spark.read.csv(fname,inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>None</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>None</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>None</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport            11   \n",
       "1  00AA  small_airport                Aero B Ranch Airport          3435   \n",
       "2  00AK  small_airport                        Lowell Field           450   \n",
       "3  00AL  small_airport                        Epps Airpark           820   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport           237   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0        NA          US      US-PA      Bensalem      00A      None   \n",
       "1        NA          US      US-KS         Leoti     00AA      None   \n",
       "2        NA          US      US-AK  Anchor Point     00AK      None   \n",
       "3        NA          US      US-AL       Harvest     00AL      None   \n",
       "4        NA          US      US-AR       Newport     None      None   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4       None                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the head of data\n",
    "airport_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing cleaning tasks here\n",
    "\n",
    "* Drop all missing values.\n",
    "\n",
    "* Drop all duplicate values.\n",
    "\n",
    "* Convert a coulmns data type.\n",
    "\n",
    "* Modify the columns letter to lowercase.\n",
    "\n",
    "* Change some of columns name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print data schema\n",
    "immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review data size\n",
    "temperature_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# review the schema of data\n",
    "temperature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### US Cities Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: double (nullable = true)\n",
      " |-- female_population: double (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_veterans: double (nullable = true)\n",
      " |-- foreign_born: double (nullable = true)\n",
      " |-- avg_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# review data schema\n",
    "demographics_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review data size\n",
    "demographics_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a copy of airport data \n",
    "airport_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# review data info\n",
    "airport_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Since we are interested in the flow of immigration in the united states, the below fact and dimension tables will serve as  clear evidence to explain our aim and this schema can be used by data analysts and other relevant business professional to gain deeper insight into various immigration figures, trends and statistics recorded historically:\n",
    "\n",
    "![title](Schema.png)\n",
    "\n",
    "\n",
    "To made conceptual data model, I used dbdiagram tool to create it on this URL : https://dbdiagram.io/home?utm_source=dbdiagram, it's free, simple tool to draw ER diagrams by just writing code.\n",
    "Designed for developers and data analysts.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The steps that necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1- Load the data into staging tables.\n",
    "\n",
    "2- Create Dimenision tables.\n",
    "\n",
    "3- Create Fact table.\n",
    "\n",
    "4- Write data into parquet files.\n",
    "\n",
    "5- Perform data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the immigration & states dataset to immi_table dataframe\n",
    "immigration_df.createOrReplaceTempView(\"fact_immigration\")\n",
    "us_states.createOrReplaceTempView(\"states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|i94addr|        state|\n",
      "+-------+-------------+\n",
      "|     AL|      Alabama|\n",
      "|     MI|     Michigan|\n",
      "|     MA|Massachusetts|\n",
      "|     MA|Massachusetts|\n",
      "|     MI|     Michigan|\n",
      "|     NJ|   New Jersey|\n",
      "|     NJ|   New Jersey|\n",
      "|     NY|     New York|\n",
      "|     NY|     New York|\n",
      "|     NY|     New York|\n",
      "|     MO|     Missouri|\n",
      "|     MA|Massachusetts|\n",
      "|     MA|Massachusetts|\n",
      "|     MA|Massachusetts|\n",
      "|     NJ|   New Jersey|\n",
      "|     NY|     New York|\n",
      "|     TX|        Texas|\n",
      "|     CT|  Connecticut|\n",
      "|     CT|  Connecticut|\n",
      "|     NJ|   New Jersey|\n",
      "+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join immigration & states dataset to display the full name of state\n",
    "spark.sql(\"\"\"\n",
    "SELECT f.i94addr, s.state\n",
    "FROM fact_immigration AS f\n",
    "INNER JOIN states AS s\n",
    "ON f.i94addr = s.code\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join immigration & states dataset to display the full name of state\n",
    "spark.sql(\"\"\"\n",
    "SELECT f.*, s.state\n",
    "FROM fact_immigration AS f\n",
    "INNER JOIN states AS s\n",
    "ON f.i94addr = s.code\n",
    "\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+\n",
      "|i94addr|               state|count(state)|\n",
      "+-------+--------------------+------------+\n",
      "|     FL|             Florida|      621701|\n",
      "|     NY|            New York|      553677|\n",
      "|     CA|          California|      470386|\n",
      "|     HI|              Hawaii|      168764|\n",
      "|     TX|               Texas|      134321|\n",
      "|     NV|              Nevada|      114609|\n",
      "|     IL|            Illinois|       82126|\n",
      "|     NJ|          New Jersey|       76531|\n",
      "|     MA|       Massachusetts|       70486|\n",
      "|     WA|          Washington|       55792|\n",
      "|     GA|             Georgia|       44663|\n",
      "|     MI|            Michigan|       32101|\n",
      "|     VA|            Virginia|       31399|\n",
      "|     PA|        Pennsylvania|       30293|\n",
      "|     DC|District of Columbia|       28228|\n",
      "|     NE|            Nebraska|       26574|\n",
      "|     MD|            Maryland|       25360|\n",
      "|     NC|      North Carolina|       23375|\n",
      "|     LA|           Louisiana|       22655|\n",
      "|     AZ|             Arizona|       20218|\n",
      "+-------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the fact immigration after add state full name\n",
    "spark.sql(\"\"\"\n",
    "SELECT  i94addr,state, count(state)\n",
    "FROM fact_immigration\n",
    "GROUP BY 1,2\n",
    "ORDER BY 3 DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------+\n",
      "|count(state)|count(i94addr)|count(cicid)|\n",
      "+------------+--------------+------------+\n",
      "|     2823040|       2823040|     2823040|\n",
      "+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the count immigration of dataset \n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(state), COUNT(i94addr),COUNT(cicid)\n",
    "FROM fact_immigration\n",
    "WHERE state IS NOT NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94mode|count(1)|\n",
      "+-------+--------+\n",
      "|    1.0| 2760441|\n",
      "|    3.0|   48410|\n",
      "|    2.0|    7917|\n",
      "|    9.0|    6272|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review i94mode content\n",
    "spark.sql(\"\"\"\n",
    "SELECT i94mode , COUNT(*)\n",
    "FROM fact_immigration\n",
    "GROUP BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|i94mode|transport_mode|\n",
      "+-------+--------------+\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "|    1.0|           Air|\n",
      "+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review data if we repace Mode of transportation to be (1 = Air, 2 = Sea, 3 = Land, 9 = Not reported)\n",
    "spark.sql(\"\"\" SELECT i94mode , CASE\n",
    "                             WHEN i94mode = 1.0 THEN 'Air'\n",
    "                             WHEN i94mode = 2.0 THEN 'Sea'\n",
    "                             WHEN i94mode = 3.0 THEN 'Land'\n",
    "                             WHEN i94mode = 3.0 THEN 'Not Reported'\n",
    "                             ELSE 'N/A' END AS transport_mode\n",
    "FROM fact_immigration                            \n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repace Mode of transportation to be (1 = Air, 2 = Sea, 3 = Land, 9 = Not reported)\n",
    "spark.sql(\"\"\" SELECT * , CASE\n",
    "                             WHEN i94mode = 1.0 THEN 'Air'\n",
    "                             WHEN i94mode = 2.0 THEN 'Sea'\n",
    "                             WHEN i94mode = 3.0 THEN 'Land'\n",
    "                             WHEN i94mode = 3.0 THEN 'Not Reported'\n",
    "                             ELSE 'N/A' END AS transport_mode\n",
    "FROM fact_immigration                            \n",
    "\n",
    "\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the data not via Air \\ as 1 = Air\n",
    "#spark.sql(\"\"\"\n",
    "#SELECT *\n",
    "#FROM fact_immigration\n",
    "#WHERE i94mode = 1.0\n",
    "#\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2823040|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the change\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM fact_immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     F| 1183006|\n",
      "|  null|  384629|\n",
      "|     M| 1254637|\n",
      "|     U|     294|\n",
      "|     X|     474|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review gender content\n",
    "spark.sql(\"\"\"\n",
    "SELECT gender, COUNT(*)\n",
    "FROM fact_immigration\n",
    "GROUP BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep just male and female travellers on data as there incorrect values\n",
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM fact_immigration\n",
    "WHERE gender IN ('F','M')\n",
    "\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2437643|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the change\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM fact_immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert arrdate to timedate\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date\n",
    "FROM fact_immigration\n",
    "\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|arrival_date|\n",
      "+------------+\n",
      "|  2016-04-07|\n",
      "|  2016-04-01|\n",
      "|  2016-04-01|\n",
      "|  2016-04-01|\n",
      "|  2016-04-01|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the change\n",
    "spark.sql(\"\"\"\n",
    "SELECT arrival_date\n",
    "FROM fact_immigration\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the departure dates to useable date\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, CASE\n",
    "               WHEN depdate >= 1.0 THEN date_add(to_date('1960-01-01'), depdate )\n",
    "               WHEN depdate IS NULL THEN NULL\n",
    "               ELSE 'N/A' END AS departure_date\n",
    "FROM fact_immigration               \n",
    "\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|departure_date|\n",
      "+--------------+\n",
      "|          null|\n",
      "|    2016-08-25|\n",
      "|    2016-04-05|\n",
      "|    2016-04-05|\n",
      "|    2016-04-17|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the change\n",
    "spark.sql(\"\"\"\n",
    "SELECT departure_date\n",
    "FROM fact_immigration\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     137|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the count in case departure date < arrival date\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM fact_immigration\n",
    "WHERE departure_date <= arrival_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop incorrect date in case departure date < arrival date\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM fact_immigration\n",
    "WHERE departure_date >= arrival_date\n",
    "\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "+------------+--------------+\n",
      "|arrival_date|departure_date|\n",
      "+------------+--------------+\n",
      "|  2016-04-01|    2016-08-25|\n",
      "|  2016-04-01|    2016-04-05|\n",
      "|  2016-04-01|    2016-04-05|\n",
      "|  2016-04-01|    2016-04-17|\n",
      "|  2016-04-01|    2016-05-04|\n",
      "+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the change\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM fact_immigration\n",
    "WHERE departure_date <= arrival_date\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT arrival_date, departure_date\n",
    "FROM fact_immigration\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|max(biryear)|min(biryear)|\n",
      "+------------+------------+\n",
      "|      2016.0|      1916.0|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# review biryear content\n",
    "spark.sql(\"\"\"\n",
    "SELECT MAX(biryear), MIN(biryear)\n",
    "FROM fact_immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|55.0|\n",
      "|58.0|\n",
      "|56.0|\n",
      "|62.0|\n",
      "|49.0|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# review if we add new column for age \n",
    "spark.sql(\"\"\"\n",
    "SELECT (2016-biryear) AS age\n",
    "FROM fact_immigration\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the new column to immigration dataset to display the age\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, (2016-biryear) AS age\n",
    "FROM fact_immigration\n",
    "\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "| age|count(age)|\n",
      "+----+----------+\n",
      "|30.0|     55173|\n",
      "|31.0|     53413|\n",
      "|33.0|     53019|\n",
      "|34.0|     52722|\n",
      "|32.0|     52617|\n",
      "|35.0|     51788|\n",
      "|29.0|     51522|\n",
      "|36.0|     50720|\n",
      "|40.0|     50437|\n",
      "|28.0|     49761|\n",
      "|37.0|     49690|\n",
      "|38.0|     47915|\n",
      "|41.0|     47091|\n",
      "|39.0|     46995|\n",
      "|45.0|     46720|\n",
      "|42.0|     46607|\n",
      "|44.0|     46534|\n",
      "|43.0|     46161|\n",
      "|27.0|     45683|\n",
      "|50.0|     45154|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the change\n",
    "spark.sql(\"\"\"\n",
    "SELECT age,COUNT(age)\n",
    "FROM fact_immigration\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|i94visa|count(i94visa)|\n",
      "+-------+--------------+\n",
      "|    1.0|        380214|\n",
      "|    3.0|         27459|\n",
      "|    2.0|       1919532|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review Visa content\n",
    "spark.sql(\"\"\"\n",
    "SELECT i94visa , COUNT(i94visa)\n",
    "FROM fact_immigration\n",
    "GROUP BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-------------+--------------+------------+--------------+----+---------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|        state|transport_mode|arrival_date|departure_date| age|visa_type|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-------------+--------------+------------+--------------+----+---------+\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|     Michigan|           Air|  2016-04-01|    2016-08-25|55.0|  Tourism|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|20549.0|  58.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1958.0|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|Massachusetts|           Air|  2016-04-01|    2016-04-05|58.0| Business|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20549.0|  56.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1960.0|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|Massachusetts|           Air|  2016-04-01|    2016-04-05|56.0| Business|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20561.0|  62.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1954.0|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|Massachusetts|           Air|  2016-04-01|    2016-04-17|62.0|  Tourism|\n",
      "| 30.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NJ|20578.0|  49.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1967.0|09302016|     M|  null|     OS|9.247020943E10|00089|      B2|   New Jersey|           Air|  2016-04-01|    2016-05-04|49.0|  Tourism|\n",
      "| 31.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NY|20611.0|  43.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1973.0|09302016|     M|  null|     OS|9.247128923E10|00089|      B2|     New York|           Air|  2016-04-01|    2016-06-06|43.0|  Tourism|\n",
      "| 33.0|2016.0|   4.0| 101.0| 101.0|    HOU|20545.0|    1.0|     TX|20554.0|  53.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1963.0|09302016|     F|  null|     TK|9.250930163E10|00033|      B2|        Texas|           Air|  2016-04-01|    2016-04-10|53.0|  Tourism|\n",
      "| 36.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20561.0|  37.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1979.0|09302016|     M|  null|     TK|9.250625823E10|00001|      B2|   New Jersey|           Air|  2016-04-01|    2016-04-17|37.0|  Tourism|\n",
      "| 37.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20567.0|  49.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1967.0|09302016|     F|  null|     AZ|9.247561783E10|00608|      B2|   New Jersey|           Air|  2016-04-01|    2016-04-23|49.0|  Tourism|\n",
      "| 38.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20575.0|  33.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1983.0|09302016|     M|  null|     AZ|9.248609253E10|00608|      B2|     New York|           Air|  2016-04-01|    2016-05-01|33.0|  Tourism|\n",
      "| 39.0|2016.0|   4.0| 101.0| 101.0|    MIA|20545.0|    1.0|     FL|20574.0|  65.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1951.0|09302016|     F|  null|     TK|9.250766263E10|00077|      B2|      Florida|           Air|  2016-04-01|    2016-04-30|65.0|  Tourism|\n",
      "| 40.0|2016.0|   4.0| 101.0| 101.0|    CHI|20545.0|    1.0|     IL|20554.0|  35.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1981.0|09302016|     M|  null|     OS|9.248056223E10|00065|      B1|     Illinois|           Air|  2016-04-01|    2016-04-10|35.0| Business|\n",
      "| 41.0|2016.0|   4.0| 101.0| 101.0|    CHI|20545.0|    1.0|     IL|20562.0|  32.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1984.0|09302016|     M|  null|     OS|9.247780953E10|00065|      B1|     Illinois|           Air|  2016-04-01|    2016-04-18|32.0| Business|\n",
      "| 42.0|2016.0|   4.0| 101.0| 101.0|    CHI|20545.0|    1.0|     IL|20580.0|  38.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1978.0|09302016|     M|  null|     TK|9.250621703E10|00005|      B1|     Illinois|           Air|  2016-04-01|    2016-05-06|38.0| Business|\n",
      "| 47.0|2016.0|   4.0| 101.0| 110.0|    NYC|20545.0|    1.0|     NJ|20665.0|  28.0|    3.0|  1.0|20160401|     HLS| null|      G|      O|   null|      M| 1988.0|     D/S|     F|  null|     AY|9.249397773E10|00005|      F1|   New Jersey|           Air|  2016-04-01|    2016-07-30|28.0|    Stady|\n",
      "| 48.0|2016.0|   4.0| 101.0| 117.0|    NYC|20545.0|    1.0|     NY|20572.0|  68.0|    2.0|  1.0|20160401|     FLR| null|      G|      O|   null|      M| 1948.0|09302016|     M|  null|     AA|9.247360483E10|00199|      B2|     New York|           Air|  2016-04-01|    2016-04-28|68.0|  Tourism|\n",
      "| 49.0|2016.0|   4.0| 101.0| 117.0|    NYC|20545.0|    1.0|     NY|20572.0|  61.0|    2.0|  1.0|20160401|     FLR| null|      G|      O|   null|      M| 1955.0|09302016|     F|  null|     AA|9.247342413E10|00199|      B2|     New York|           Air|  2016-04-01|    2016-04-28|61.0|  Tourism|\n",
      "| 50.0|2016.0|   4.0| 101.0| 117.0|    NYC|20545.0|    1.0|     MI|20563.0|  41.0|    2.0|  1.0|20160401|     NPL| null|      G|      O|   null|      M| 1975.0|09302016|     F|  null|     AZ|9.250367503E10|00610|      B2|     Michigan|           Air|  2016-04-01|    2016-04-19|41.0|  Tourism|\n",
      "| 51.0|2016.0|   4.0| 101.0| 117.0|    MIA|20545.0|    1.0|     FL|20555.0|  45.0|    2.0|  1.0|20160401|     RME| null|      G|      O|   null|      M| 1971.0|09302016|     F|  null|     AA|9.248626763E10|00207|      B2|      Florida|           Air|  2016-04-01|    2016-04-11|45.0|  Tourism|\n",
      "| 52.0|2016.0|   4.0| 101.0| 112.0|    NYC|20545.0|    1.0|     NY|20558.0|  54.0|    2.0|  1.0|20160401|     BRL| null|      G|      O|   null|      M| 1962.0|09302016|     M|  null|     AB|9.246991063E10|07450|      B2|     New York|           Air|  2016-04-01|    2016-04-14|54.0|  Tourism|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-------------+--------------+------------+--------------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review if we repace Visa categories to be (1 = Business, 2 = Tourism, 3 = Stady)\n",
    "spark.sql(\"\"\" SELECT * , CASE\n",
    "                             WHEN i94visa = 1.0 THEN 'Business'\n",
    "                             WHEN i94visa = 2.0 THEN 'Tourism'\n",
    "                             WHEN i94visa = 3.0 THEN 'Study'\n",
    "                             ELSE 'N/A' END AS visa_type\n",
    "FROM fact_immigration                            \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repace Visa categories to be (1 = Business, 2 = Tourism, 3 = Stady)\n",
    "spark.sql(\"\"\" SELECT * , CASE\n",
    "                             WHEN i94visa = 1.0 THEN 'Business'\n",
    "                             WHEN i94visa = 2.0 THEN 'Tourism'\n",
    "                             WHEN i94visa = 3.0 THEN 'Study'\n",
    "                             ELSE 'N/A' END AS visa_type\n",
    "FROM fact_immigration\n",
    "\"\"\").createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|visa_type|\n",
      "+---------+\n",
      "|  Tourism|\n",
      "| Business|\n",
      "| Business|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "| Business|\n",
      "| Business|\n",
      "| Business|\n",
      "|    Study|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "|  Tourism|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review Visa content after change\n",
    "spark.sql(\"\"\"\n",
    "SELECT visa_type\n",
    "FROM fact_immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|visa_type|count(visa_type)|\n",
      "+---------+----------------+\n",
      "|    Study|           27459|\n",
      "|  Tourism|         1919532|\n",
      "| Business|          380214|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review Visa content after change\n",
    "spark.sql(\"\"\"\n",
    "SELECT  visa_type, COUNT(visa_type)\n",
    "FROM fact_immigration\n",
    "GROUP BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2327205|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review Visa content after change\n",
    "spark.sql(\"\"\"\n",
    "SELECT  COUNT(*)\n",
    "FROM fact_immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the immigration fact table into spark dataframe\n",
    "fact_immigration_table = spark.sql(\"\"\"\n",
    "SELECT cicid,\n",
    "       state,\n",
    "       i94cit AS country_code,\n",
    "       i94port AS port,\n",
    "       arrival_date,\n",
    "       departure_date,\n",
    "       biryear,\n",
    "       gender,\n",
    "       age,\n",
    "       visa_type,\n",
    "       transport_mode\n",
    "       \n",
    "FROM fact_immigration       \n",
    "\"\"\")\n",
    "fact_immigration_table.createOrReplaceTempView(\"fact_immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------------+----+------------+--------------+-------+------+----+---------+--------------+\n",
      "|cicid|        state|country_code|port|arrival_date|departure_date|biryear|gender| age|visa_type|transport_mode|\n",
      "+-----+-------------+------------+----+------------+--------------+-------+------+----+---------+--------------+\n",
      "| 15.0|     Michigan|       101.0| WAS|  2016-04-01|    2016-08-25| 1961.0|     M|55.0|  Tourism|           Air|\n",
      "| 27.0|Massachusetts|       101.0| BOS|  2016-04-01|    2016-04-05| 1958.0|     M|58.0| Business|           Air|\n",
      "| 28.0|Massachusetts|       101.0| ATL|  2016-04-01|    2016-04-05| 1960.0|     F|56.0| Business|           Air|\n",
      "| 29.0|Massachusetts|       101.0| ATL|  2016-04-01|    2016-04-17| 1954.0|     M|62.0|  Tourism|           Air|\n",
      "| 30.0|   New Jersey|       101.0| ATL|  2016-04-01|    2016-05-04| 1967.0|     M|49.0|  Tourism|           Air|\n",
      "| 31.0|     New York|       101.0| ATL|  2016-04-01|    2016-06-06| 1973.0|     M|43.0|  Tourism|           Air|\n",
      "| 33.0|        Texas|       101.0| HOU|  2016-04-01|    2016-04-10| 1963.0|     F|53.0|  Tourism|           Air|\n",
      "| 36.0|   New Jersey|       101.0| NYC|  2016-04-01|    2016-04-17| 1979.0|     M|37.0|  Tourism|           Air|\n",
      "| 37.0|   New Jersey|       101.0| NYC|  2016-04-01|    2016-04-23| 1967.0|     F|49.0|  Tourism|           Air|\n",
      "| 38.0|     New York|       101.0| NYC|  2016-04-01|    2016-05-01| 1983.0|     M|33.0|  Tourism|           Air|\n",
      "| 39.0|      Florida|       101.0| MIA|  2016-04-01|    2016-04-30| 1951.0|     F|65.0|  Tourism|           Air|\n",
      "| 40.0|     Illinois|       101.0| CHI|  2016-04-01|    2016-04-10| 1981.0|     M|35.0| Business|           Air|\n",
      "| 41.0|     Illinois|       101.0| CHI|  2016-04-01|    2016-04-18| 1984.0|     M|32.0| Business|           Air|\n",
      "| 42.0|     Illinois|       101.0| CHI|  2016-04-01|    2016-05-06| 1978.0|     M|38.0| Business|           Air|\n",
      "| 47.0|   New Jersey|       101.0| NYC|  2016-04-01|    2016-07-30| 1988.0|     F|28.0|    Study|           Air|\n",
      "| 48.0|     New York|       101.0| NYC|  2016-04-01|    2016-04-28| 1948.0|     M|68.0|  Tourism|           Air|\n",
      "| 49.0|     New York|       101.0| NYC|  2016-04-01|    2016-04-28| 1955.0|     F|61.0|  Tourism|           Air|\n",
      "| 50.0|     Michigan|       101.0| NYC|  2016-04-01|    2016-04-19| 1975.0|     F|41.0|  Tourism|           Air|\n",
      "| 51.0|      Florida|       101.0| MIA|  2016-04-01|    2016-04-11| 1971.0|     F|45.0|  Tourism|           Air|\n",
      "| 52.0|     New York|       101.0| NYC|  2016-04-01|    2016-04-14| 1962.0|     M|54.0|  Tourism|           Air|\n",
      "+-----+-------------+------------+----+------------+--------------+-------+------+----+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM fact_immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2327205|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the fact_immigration_table \n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM fact_immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all distinct dates from arrival and departure dates to create dimension table\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT DISTINCT arrdate AS date\n",
    "FROM fact_immigration\n",
    "UNION\n",
    "SELECT DISTINCT departure_date AS date\n",
    "FROM fact_immigration\n",
    "WHERE departure_date IS NOT NULL\n",
    "\"\"\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, week, day from the date and insert all values to dim_time table\n",
    "dim_time_table = spark.sql(\"\"\"\n",
    "SELECT date,\n",
    "       YEAR(date) AS year,\n",
    "       MONTH(date) AS month,\n",
    "       DAY(date) AS day,\n",
    "       WEEKOFYEAR(date) AS week,\n",
    "       WEEKDAY(date) AS weekday,\n",
    "       DAYOFYEAR(date) AS year_day\n",
    "FROM dim_time\n",
    "ORDER BY date ASC\n",
    "\"\"\")\n",
    "dim_time_table.createOrReplaceTempView(\"dim_time_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----+-------+--------+\n",
      "|      date|year|month|day|week|weekday|year_day|\n",
      "+----------+----+-----+---+----+-------+--------+\n",
      "|2016-04-02|2016|    4|  2|  13|      5|      93|\n",
      "|2016-04-03|2016|    4|  3|  13|      6|      94|\n",
      "|2016-04-04|2016|    4|  4|  14|      0|      95|\n",
      "|2016-04-05|2016|    4|  5|  14|      1|      96|\n",
      "|2016-04-06|2016|    4|  6|  14|      2|      97|\n",
      "|2016-04-07|2016|    4|  7|  14|      3|      98|\n",
      "|2016-04-08|2016|    4|  8|  14|      4|      99|\n",
      "|2016-04-09|2016|    4|  9|  14|      5|     100|\n",
      "|2016-04-10|2016|    4| 10|  14|      6|     101|\n",
      "|2016-04-11|2016|    4| 11|  15|      0|     102|\n",
      "|2016-04-12|2016|    4| 12|  15|      1|     103|\n",
      "|2016-04-13|2016|    4| 13|  15|      2|     104|\n",
      "|2016-04-14|2016|    4| 14|  15|      3|     105|\n",
      "|2016-04-15|2016|    4| 15|  15|      4|     106|\n",
      "|2016-04-16|2016|    4| 16|  15|      5|     107|\n",
      "|2016-04-17|2016|    4| 17|  15|      6|     108|\n",
      "|2016-04-18|2016|    4| 18|  16|      0|     109|\n",
      "|2016-04-19|2016|    4| 19|  16|      1|     110|\n",
      "|2016-04-20|2016|    4| 20|  16|      2|     111|\n",
      "|2016-04-21|2016|    4| 21|  16|      3|     112|\n",
      "+----------+----+-----+---+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the dim_time_table \n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_time_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to extract date \n",
    "#dim_time_table = dim_time.withColumn('year',year('date'))\n",
    "#dim_time_table = dim_time.withColumn('month',month('date'))\n",
    "#dim_time_table = dim_time.withColumn('week',weekofyear('date'))\n",
    "#dim_time_table = dim_time.withColumn('day',dayofmonth('date'))\n",
    "#dim_time_table = dim_time.withColumn('weekday',dayofweek('date'))\n",
    "#dim_time_table = dim_time.withColumn('yearday',dayofyear('date'))\n",
    "#dim_time_table.createOrReplaceTempView(\"dim_time_table\")\n",
    "#dim_time_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the temperature dataset to temp_table dataframe\n",
    "temperature_df.createOrReplaceTempView(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only data for the U.S\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_temperature\n",
    "WHERE country = 'United States'\n",
    "\"\"\").createOrReplaceTempView(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      country|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the change\n",
    "spark.sql(\"\"\"\n",
    "SELECT country\n",
    "FROM dim_temperature\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|   City|      Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|1820-01-01 00:00:00|2.1010000000000004|                        3.217|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-02-01 00:00:00|             6.926|                        2.853|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-03-01 00:00:00|            10.767|                        2.395|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-04-01 00:00:00|17.988999999999994|                        2.202|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-05-01 00:00:00|            21.809|                        2.036|Abilene|United States|  32.95N|  100.53W|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the change\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_temperature\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temperature dimontion table into spark datafram\n",
    "dim_temperature_table = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "      dt AS date,\n",
    "      City AS city,\n",
    "      Country AS country,\n",
    "      AVG(AverageTemperature) AS avg_temp,\n",
    "      AVG(AverageTemperatureUncertainty) AS avg_temp_uncertainty\n",
    "FROM dim_temperature \n",
    "GROUP BY 1,2,3\n",
    "\"\"\")\n",
    "dim_temperature_table.createOrReplaceTempView(\"dim_temperature_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------+------------------+--------------------+\n",
      "|               date|        city|      country|          avg_temp|avg_temp_uncertainty|\n",
      "+-------------------+------------+-------------+------------------+--------------------+\n",
      "|1764-05-01 00:00:00|   Allentown|United States|              null|                null|\n",
      "|1779-06-01 00:00:00|   Allentown|United States|              null|                null|\n",
      "|1910-02-01 00:00:00|     Abilene|United States|             5.586|               0.847|\n",
      "|1892-02-01 00:00:00|   Ann Arbor|United States|            -2.495| 0.42200000000000004|\n",
      "|1875-11-01 00:00:00|      Arvada|United States|             -3.96|  0.6970000000000001|\n",
      "|1763-07-01 00:00:00|      Aurora|United States|              null|                null|\n",
      "|1795-11-01 00:00:00| Baton Rouge|United States|              null|                null|\n",
      "|1830-07-01 00:00:00|    Bellevue|United States|              null|                null|\n",
      "|2001-02-01 00:00:00|    Bellevue|United States|1.7760000000000002|               0.245|\n",
      "|1751-11-01 00:00:00|  Bridgeport|United States|              null|                null|\n",
      "|1834-01-01 00:00:00| Brownsville|United States|              null|                null|\n",
      "|1745-10-01 00:00:00|   Cambridge|United States|              null|                null|\n",
      "|1745-12-01 00:00:00|   Cambridge|United States|              null|                null|\n",
      "|1760-07-01 00:00:00|   Cambridge|United States|              null|                null|\n",
      "|1761-11-01 00:00:00|  Cape Coral|United States|              null|                null|\n",
      "|1749-02-01 00:00:00|Cedar Rapids|United States|              null|                null|\n",
      "|1748-02-01 00:00:00|   Charlotte|United States|              null|                null|\n",
      "|1749-08-01 00:00:00|   Charlotte|United States|              null|                null|\n",
      "|1748-09-01 00:00:00| Chattanooga|United States|              null|                null|\n",
      "|1815-10-01 00:00:00|  Cape Coral|United States|              null|                null|\n",
      "+-------------------+------------+-------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the dim_temperature_table \n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_temperature_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the time from date column\n",
    "dim_temperature_table = dim_temperature_table.withColumn(\"date\",date_format(col(\"date\"), \"YYYY-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------------+--------------------+\n",
      "|      date|        city|      country|          avg_temp|avg_temp_uncertainty|\n",
      "+----------+------------+-------------+------------------+--------------------+\n",
      "|1764-05-01|   Allentown|United States|              null|                null|\n",
      "|1779-06-01|   Allentown|United States|              null|                null|\n",
      "|1910-02-01|     Abilene|United States|             5.586|               0.847|\n",
      "|1892-02-01|   Ann Arbor|United States|            -2.495| 0.42200000000000004|\n",
      "|1875-11-01|      Arvada|United States|             -3.96|  0.6970000000000001|\n",
      "|1763-07-01|      Aurora|United States|              null|                null|\n",
      "|1795-11-01| Baton Rouge|United States|              null|                null|\n",
      "|1830-07-01|    Bellevue|United States|              null|                null|\n",
      "|2001-02-01|    Bellevue|United States|1.7760000000000002|               0.245|\n",
      "|1751-11-01|  Bridgeport|United States|              null|                null|\n",
      "|1834-01-01| Brownsville|United States|              null|                null|\n",
      "|1745-10-01|   Cambridge|United States|              null|                null|\n",
      "|1745-12-01|   Cambridge|United States|              null|                null|\n",
      "|1760-07-01|   Cambridge|United States|              null|                null|\n",
      "|1761-11-01|  Cape Coral|United States|              null|                null|\n",
      "|1749-02-01|Cedar Rapids|United States|              null|                null|\n",
      "|1748-02-01|   Charlotte|United States|              null|                null|\n",
      "|1749-08-01|   Charlotte|United States|              null|                null|\n",
      "|1748-09-01| Chattanooga|United States|              null|                null|\n",
      "|1815-10-01|  Cape Coral|United States|              null|                null|\n",
      "+----------+------------+-------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the dim_temperature_table\n",
    "dim_temperature_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the change on dim_temperature_table\n",
    "dim_temperature_table.createOrReplaceTempView(\"dim_temperature_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+------------------+--------------------+\n",
      "|      date|        city|      country|          avg_temp|avg_temp_uncertainty|\n",
      "+----------+------------+-------------+------------------+--------------------+\n",
      "|1764-05-01|   Allentown|United States|              null|                null|\n",
      "|1779-06-01|   Allentown|United States|              null|                null|\n",
      "|1910-02-01|     Abilene|United States|             5.586|               0.847|\n",
      "|1892-02-01|   Ann Arbor|United States|            -2.495| 0.42200000000000004|\n",
      "|1875-11-01|      Arvada|United States|             -3.96|  0.6970000000000001|\n",
      "|1763-07-01|      Aurora|United States|              null|                null|\n",
      "|1795-11-01| Baton Rouge|United States|              null|                null|\n",
      "|1830-07-01|    Bellevue|United States|              null|                null|\n",
      "|2001-02-01|    Bellevue|United States|1.7760000000000002|               0.245|\n",
      "|1751-11-01|  Bridgeport|United States|              null|                null|\n",
      "|1834-01-01| Brownsville|United States|              null|                null|\n",
      "|1745-10-01|   Cambridge|United States|              null|                null|\n",
      "|1745-12-01|   Cambridge|United States|              null|                null|\n",
      "|1760-07-01|   Cambridge|United States|              null|                null|\n",
      "|1761-11-01|  Cape Coral|United States|              null|                null|\n",
      "|1749-02-01|Cedar Rapids|United States|              null|                null|\n",
      "|1748-02-01|   Charlotte|United States|              null|                null|\n",
      "|1749-08-01|   Charlotte|United States|              null|                null|\n",
      "|1748-09-01| Chattanooga|United States|              null|                null|\n",
      "|1815-10-01|  Cape Coral|United States|              null|                null|\n",
      "+----------+------------+-------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the dim_temperature_table \n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_temperature_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the demographics dataset to demo_table dataframe\n",
    "demographics_df.createOrReplaceTempView(\"dim_demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demographics dimention table into spark dataframe\n",
    "dim_demographics_table = spark.sql(\"\"\"\n",
    "SELECT \n",
    "      City AS city,\n",
    "      State AS state,\n",
    "      median_age,\n",
    "      male_population,\n",
    "      female_population,\n",
    "      total_population,\n",
    "      foreign_born,\n",
    "      avg_household_size,\n",
    "      state_code,\n",
    "      Race AS race,\n",
    "      Count AS count\n",
    "FROM dim_demographics\n",
    "\"\"\")\n",
    "# Save the new demographics dataset to demo_table dataframe\n",
    "dim_demographics_table.createOrReplaceTempView(\"dim_demographics_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------+------------------+----------+--------------------+------+\n",
      "|            city|         state|median_age|male_population|female_population|total_population|foreign_born|avg_household_size|state_code|                race| count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------+------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|        40601.0|          41862.0|           82463|     30908.0|               2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|        44129.0|          49500.0|           93629|     32935.0|              2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|        38040.0|          46799.0|           84839|      8229.0|              2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|        88127.0|          87105.0|          175232|     33878.0|              3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|       138040.0|         143873.0|          281913|     86253.0|              2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|        56229.0|          62432.0|          118661|      7517.0|               2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|        38712.0|          41971.0|           80683|      8355.0|              3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|        51629.0|          56860.0|          108489|     37038.0|              3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|        41762.0|          43270.0|           85032|      3269.0|              2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|        51751.0|          58077.0|          109828|     16315.0|              2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|        41051.0|          35317.0|           76368|     13234.0|              2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|        41051.0|          35317.0|           76368|     13234.0|              2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|       741270.0|         826172.0|         1567442|    205339.0|              2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|       192354.0|         197601.0|          389955|     40270.0|              2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|       192354.0|         197601.0|          389955|     40270.0|              2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|        36850.0|          37165.0|           74015|     15365.0|              2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|       149690.0|         154695.0|          304385|     28187.0|              2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|       124305.0|         131484.0|          255789|     68427.0|              3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|        60142.0|          60829.0|          120971|     25000.0|              2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|        63278.0|          62938.0|          126216|     52281.0|              2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------+------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the dim_demographics_table\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_demographics_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the airport dataset to air_table dataframe\n",
    "airport_df.createOrReplaceTempView(\"dim_airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop is null values from air_table\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_airport\n",
    "WHERE iso_country IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView(\"dim_airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temperature dimention table into spaark dataframe\n",
    "dim_airport_table = spark.sql(\"\"\"\n",
    "SELECT\n",
    "      ident AS id,\n",
    "      type,\n",
    "      name,\n",
    "      elevation_ft,\n",
    "      iso_region AS state_code,\n",
    "      municipality,\n",
    "      iata_code\n",
    "FROM dim_airport      \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------+------------+----------+------------+---------+\n",
      "|  id|         type|                name|elevation_ft|state_code|municipality|iata_code|\n",
      "+----+-------------+--------------------+------------+----------+------------+---------+\n",
      "| 00A|     heliport|   Total Rf Heliport|          11|        PA|    Bensalem|     null|\n",
      "|00AA|small_airport|Aero B Ranch Airport|        3435|        KS|       Leoti|     null|\n",
      "|00AK|small_airport|        Lowell Field|         450|        AK|Anchor Point|     null|\n",
      "|00AL|small_airport|        Epps Airpark|         820|        AL|     Harvest|     null|\n",
      "|00AR|       closed|Newport Hospital ...|         237|        AR|     Newport|     null|\n",
      "|00AS|small_airport|      Fulton Airport|        1100|        OK|        Alex|     null|\n",
      "|00AZ|small_airport|      Cordes Airport|        3810|        AZ|      Cordes|     null|\n",
      "|00CA|small_airport|Goldstone /Gts/ A...|        3038|        CA|     Barstow|     null|\n",
      "|00CL|small_airport| Williams Ag Airport|          87|        CA|       Biggs|     null|\n",
      "|00CN|     heliport|Kitchen Creek Hel...|        3350|        CA| Pine Valley|     null|\n",
      "|00CO|       closed|          Cass Field|        4830|        CO|  Briggsdale|     null|\n",
      "|00FA|small_airport| Grass Patch Airport|          53|        FL|    Bushnell|     null|\n",
      "|00FD|     heliport|  Ringhaver Heliport|          25|        FL|   Riverview|     null|\n",
      "|00FL|small_airport|   River Oak Airport|          35|        FL|  Okeechobee|     null|\n",
      "|00GA|small_airport|    Lt World Airport|         700|        GA|    Lithonia|     null|\n",
      "|00GE|     heliport|    Caffrey Heliport|         957|        GA|       Hiram|     null|\n",
      "|00HI|     heliport|  Kaupulehu Heliport|          43|        HI| Kailua/Kona|     null|\n",
      "|00ID|small_airport|Delta Shores Airport|        2064|        ID|  Clark Fork|     null|\n",
      "|00IG|small_airport|       Goltl Airport|        3359|        KS|    McDonald|     null|\n",
      "|00II|     heliport|Bailey Generation...|         600|        IN|  Chesterton|     null|\n",
      "+----+-------------+--------------------+------------+----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "dim_airport_table = dim_airport_table.withColumn('state_code',regexp_replace('state_code', 'US-',''))\n",
    "dim_airport_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the change to air_table dataframe\n",
    "dim_airport_table.createOrReplaceTempView(\"dim_airport_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------+------------+----------+------------+---------+\n",
      "|  id|         type|                name|elevation_ft|state_code|municipality|iata_code|\n",
      "+----+-------------+--------------------+------------+----------+------------+---------+\n",
      "| 00A|     heliport|   Total Rf Heliport|          11|        PA|    Bensalem|     null|\n",
      "|00AA|small_airport|Aero B Ranch Airport|        3435|        KS|       Leoti|     null|\n",
      "|00AK|small_airport|        Lowell Field|         450|        AK|Anchor Point|     null|\n",
      "|00AL|small_airport|        Epps Airpark|         820|        AL|     Harvest|     null|\n",
      "|00AR|       closed|Newport Hospital ...|         237|        AR|     Newport|     null|\n",
      "|00AS|small_airport|      Fulton Airport|        1100|        OK|        Alex|     null|\n",
      "|00AZ|small_airport|      Cordes Airport|        3810|        AZ|      Cordes|     null|\n",
      "|00CA|small_airport|Goldstone /Gts/ A...|        3038|        CA|     Barstow|     null|\n",
      "|00CL|small_airport| Williams Ag Airport|          87|        CA|       Biggs|     null|\n",
      "|00CN|     heliport|Kitchen Creek Hel...|        3350|        CA| Pine Valley|     null|\n",
      "|00CO|       closed|          Cass Field|        4830|        CO|  Briggsdale|     null|\n",
      "|00FA|small_airport| Grass Patch Airport|          53|        FL|    Bushnell|     null|\n",
      "|00FD|     heliport|  Ringhaver Heliport|          25|        FL|   Riverview|     null|\n",
      "|00FL|small_airport|   River Oak Airport|          35|        FL|  Okeechobee|     null|\n",
      "|00GA|small_airport|    Lt World Airport|         700|        GA|    Lithonia|     null|\n",
      "|00GE|     heliport|    Caffrey Heliport|         957|        GA|       Hiram|     null|\n",
      "|00HI|     heliport|  Kaupulehu Heliport|          43|        HI| Kailua/Kona|     null|\n",
      "|00ID|small_airport|Delta Shores Airport|        2064|        ID|  Clark Fork|     null|\n",
      "|00IG|small_airport|       Goltl Airport|        3359|        KS|    McDonald|     null|\n",
      "|00II|     heliport|Bailey Generation...|         600|        IN|  Chesterton|     null|\n",
      "+----+-------------+--------------------+------------+----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the dim_airport_table\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM dim_airport_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fact_immigration_table has 2327205 rows.\n",
      "The dim_demographics_table has 2891 rows.\n",
      "The dim_time_table has 203 rows.\n",
      "The dim_airport_table has 55075 rows.\n",
      "The dim_temperature_table has 664481 rows.\n"
     ]
    }
   ],
   "source": [
    "# Count Rows check test\n",
    "# Define the table names\n",
    "table_names = [\"fact_immigration_table\",\"dim_demographics_table\",\"dim_time_table\",\"dim_airport_table\",\"dim_temperature_table\"]\n",
    "\n",
    "# Loop through each table name and perform a count check \n",
    "for table_name in table_names:\n",
    "    # Read the table into dataframe\n",
    "    table_df = spark.table(table_name)\n",
    "    # Get the count of rows\n",
    "    row_count = table_df.count()\n",
    "    # print the result\n",
    "    print(f\"The {table_name} has {row_count} rows.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fact_immigration_table has 0 duplicates!\n",
      "The dim_demographics_table has 0 duplicates!\n",
      "The dim_time_table has 0 duplicates!\n",
      "The dim_airport_table has 0 duplicates!\n",
      "The dim_temperature_table has 0 duplicates!\n"
     ]
    }
   ],
   "source": [
    "# Duplicates test\n",
    "# Define the table names\n",
    "table_names = [\"fact_immigration_table\",\"dim_demographics_table\",\"dim_time_table\",\"dim_airport_table\",\"dim_temperature_table\"]\n",
    "\n",
    "# Loop through each table name and perform null values\n",
    "for table_name in table_names:\n",
    "    # read the table into dataframe\n",
    "    #able_df = spark.table(table_name)\n",
    "    # Check for duplicates in dataframe\n",
    "    duplicate_count = table_df.groupBy(table_df.columns).agg(count(\"*\").alias(\"count\")).filter(col(\"count\") > 1).count()\n",
    "    \n",
    "    \n",
    "    #print the result\n",
    "    print(f\"The {table_name} has {duplicate_count} duplicates!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fact_immigration_table has Row(cicid=0, state=0, country_code=0, port=0, arrival_date=0, departure_date=0, biryear=1, gender=0, age=1, visa_type=0, transport_mode=0) null values!.\n",
      "the dim_demographics_table has Row(city=0, state=0, median_age=0, male_population=3, female_population=3, total_population=0, foreign_born=13, avg_household_size=16, state_code=0, race=0, count=0) null values!.\n",
      "the dim_time_table has Row(date=0, year=30, month=30, day=30, week=30, weekday=30, year_day=30) null values!.\n",
      "the dim_airport_table has Row(id=0, type=0, name=0, elevation_ft=7006, state_code=0, municipality=5676, iata_code=45886) null values!.\n",
      "the dim_temperature_table has Row(date=0, city=0, country=0, avg_temp=24832, avg_temp_uncertainty=24832) null values!.\n"
     ]
    }
   ],
   "source": [
    "# Null Values test\n",
    "# Define the table names\n",
    "table_names = [\"fact_immigration_table\",\"dim_demographics_table\",\"dim_time_table\",\"dim_airport_table\",\"dim_temperature_table\"]\n",
    "\n",
    "# Loop through each table name and perform null values\n",
    "for table_name in table_names:\n",
    "    # read the table into dataframe\n",
    "    table_df = spark.table(table_name)\n",
    "    # Check for null values in dataframe\n",
    "    null_value_count = table_df.select([count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in table_df.columns]).collect()[0]\n",
    "    # print the result\n",
    "    print(f\"the {table_name} has {null_value_count} null values!.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_immigration_table\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country_code: double (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- visa_type: string (nullable = false)\n",
      " |-- transport_mode: string (nullable = false)\n",
      "\n",
      "dim_temperature_table\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      " |-- avg_temp_uncertainty: double (nullable = true)\n",
      "\n",
      "dim_time_table\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- year_day: integer (nullable = true)\n",
      "\n",
      "dim_airport_table\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      "\n",
      "dim_demographics_table\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: double (nullable = true)\n",
      " |-- female_population: double (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- foreign_born: double (nullable = true)\n",
      " |-- avg_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema for each table\n",
    "print(\"fact_immigration_table\")\n",
    "fact_immigration_table.printSchema()\n",
    "print(\"dim_temperature_table\")\n",
    "dim_temperature_table.printSchema()\n",
    "print(\"dim_time_table\")\n",
    "dim_time_table.printSchema()\n",
    "print(\"dim_airport_table\")\n",
    "dim_airport_table.printSchema()\n",
    "print(\"dim_demographics_table\")\n",
    "dim_demographics_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the fact_immigration and dim_temperaturetables and dim_demographics tables toghater\n",
    "#spark.sql(\"\"\"\n",
    "#SELECT f.state, f.age, f.visa_type, t.avg_temp,f.transport_mode, d.total_population\n",
    "#FROM fact_immigration_table AS f\n",
    "#JOIN dim_temperature_table AS t\n",
    "#ON f.state = t.city\n",
    "#JOIN dim_demographics_table AS d\n",
    "#ON f.state = d.state\n",
    "#JOIN dim_time_table AS td\n",
    "#ON t.date = td.date\n",
    "#\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+---------+--------------------+--------------+\n",
      "|     state|arrival_date| age|visa_type|            avg_temp|transport_mode|\n",
      "+----------+------------+----+---------+--------------------+--------------+\n",
      "|Washington|  2016-04-01|52.0|  Tourism|                null|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|  14.584000000000001|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|  10.739999999999998|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              18.563|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              13.906|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|               3.209|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|-0.43200000000000005|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|  12.549000000000001|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              14.558|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|                4.25|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|-0.04000000000000...|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|  1.1889999999999998|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|               1.621|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              22.136|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              23.823|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              19.092|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              18.234|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              20.534|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|              21.033|           Air|\n",
      "|Washington|  2016-04-01|52.0|  Tourism|  0.5029999999999999|           Air|\n",
      "+----------+------------+----+---------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join the fact_immigration and dim_temperaturetables toghater\n",
    "spark.sql(\"\"\"\n",
    "SELECT f.state, f.arrival_date, f.age, f.visa_type, t.avg_temp,f.transport_mode\n",
    "FROM fact_immigration_table AS f\n",
    "JOIN dim_temperature_table AS t\n",
    "ON f.state = t.city\n",
    "WHERE avg_temp >= 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the fact_immigration and dim_temperaturetables and dim_demographics tables toghater\n",
    "#spark.sql(\"\"\"\n",
    "#SELECT f.state, f.age, COUNT(f.visa_type), t.avg_temp,f.transport_mode, d.total_population\n",
    "#FROM fact_immigration_table AS f\n",
    "#JOIN dim_temperature_table AS t\n",
    "#ON f.state = t.city\n",
    "#JOIN dim_demographics_table AS d\n",
    "#ON f.state = d.state\n",
    "#JOIN dim_time_table AS td\n",
    "#ON t.date = td.date\n",
    "#\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------+---------------+-----------------+----------------+\n",
      "|   state| age|gender|male_population|female_population|total_population|\n",
      "+--------+----+------+---------------+-----------------+----------------+\n",
      "|Michigan|55.0|     M|        31369.0|          41808.0|           73177|\n",
      "|Michigan|55.0|     M|        35967.0|          39310.0|           75277|\n",
      "|Michigan|55.0|     M|        58789.0|          58281.0|          117070|\n",
      "|Michigan|55.0|     M|        36356.0|          37076.0|           73432|\n",
      "|Michigan|55.0|     M|        64063.0|          71293.0|          135356|\n",
      "|Michigan|55.0|     M|        37175.0|          38865.0|           76040|\n",
      "|Michigan|55.0|     M|        36356.0|          37076.0|           73432|\n",
      "|Michigan|55.0|     M|        31369.0|          41808.0|           73177|\n",
      "|Michigan|55.0|     M|        31369.0|          41808.0|           73177|\n",
      "|Michigan|55.0|     M|        64063.0|          71293.0|          135356|\n",
      "|Michigan|55.0|     M|        37742.0|          44253.0|           81995|\n",
      "|Michigan|55.0|     M|        54333.0|          59777.0|          114110|\n",
      "|Michigan|55.0|     M|        45369.0|          49264.0|           94633|\n",
      "|Michigan|55.0|     M|        95669.0|          99430.0|          195099|\n",
      "|Michigan|55.0|     M|        54333.0|          59777.0|          114110|\n",
      "|Michigan|55.0|     M|        64985.0|          67077.0|          132062|\n",
      "|Michigan|55.0|     M|       319265.0|         357859.0|          677124|\n",
      "|Michigan|55.0|     M|        54333.0|          59777.0|          114110|\n",
      "|Michigan|55.0|     M|        31369.0|          41808.0|           73177|\n",
      "|Michigan|55.0|     M|        37527.0|          43805.0|           81332|\n",
      "+--------+----+------+---------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# review the age rate and city and male & female \n",
    "spark.sql(\"\"\"\n",
    "SELECT f.state, f.age,f.gender, d.male_population,d.female_population,total_population\n",
    "FROM fact_immigration_table AS f\n",
    "JOIN dim_demographics_table AS d\n",
    "ON f.state = d.state\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-----------------------+\n",
      "|         state|          avg(age)|count(total_population)|\n",
      "+--------------+------------------+-----------------------+\n",
      "|    California| 41.87245177943538|              266079008|\n",
      "|       Florida|40.611540487978736|              114607944|\n",
      "|         Texas|44.569836635127146|               29545152|\n",
      "|      New York| 40.03067495212076|               25122582|\n",
      "|      Illinois| 43.58689612594536|                5895890|\n",
      "|        Nevada|  42.2622057243333|                4606650|\n",
      "|    Washington|   41.712776456866|                3789470|\n",
      "| Massachusetts| 43.00791177175737|                3741387|\n",
      "|    New Jersey|44.917880202795374|                3698901|\n",
      "|      Michigan| 43.19893449570166|                1957383|\n",
      "|      Virginia|45.645065877924175|                1822310|\n",
      "|       Georgia|45.451668230801594|                1699555|\n",
      "|North Carolina|44.777914740626606|                1362900|\n",
      "|       Arizona| 45.67388955582233|                1332800|\n",
      "|      Maryland|  46.1049066189961|                1062850|\n",
      "|      Colorado| 44.28886993603412|                 938000|\n",
      "|  Pennsylvania| 43.72602207368257|                 849156|\n",
      "|     Louisiana|43.724299593312246|                 708160|\n",
      "|          Ohio|  43.7930694464009|                 695702|\n",
      "|        Hawaii| 41.14324211375787|                 693935|\n",
      "+--------------+------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# review the highest city and age rate & total Population\n",
    "spark.sql(\"\"\"\n",
    "SELECT f.state, AVG(f.age),COUNT(d.total_population)\n",
    "FROM fact_immigration_table AS f\n",
    "JOIN dim_demographics_table AS d\n",
    "ON f.state = d.state\n",
    "GROUP BY 1\n",
    "ORDER BY 3 DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most average of age 41.8 years are immigration to California City and it's the most population city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-----------------------+\n",
      "|         state|          avg(age)|count(total_population)|\n",
      "+--------------+------------------+-----------------------+\n",
      "|    California| 41.71518952597919|               49257416|\n",
      "|         Texas| 42.54297853451283|                8610147|\n",
      "|       Florida| 42.98742121424339|                8241972|\n",
      "|      New York|  42.8458540042523|                2438208|\n",
      "|      Illinois|42.239607959022855|                1847664|\n",
      "|    Washington| 41.24383139136395|                 992120|\n",
      "| Massachusetts| 42.32705882352941|                 909075|\n",
      "|      Michigan| 41.33250414593698|                 857466|\n",
      "|        Nevada| 42.48653597587247|                 835560|\n",
      "|    New Jersey|43.479519439977295|                 602547|\n",
      "|North Carolina| 42.76927868444002|                 561890|\n",
      "|       Georgia| 42.68189629030628|                 544115|\n",
      "|      Virginia|43.142140468227424|                 376740|\n",
      "|       Arizona| 43.26148016049933|                 358880|\n",
      "|          Ohio|42.121995390187685|                 297626|\n",
      "|      Colorado| 42.50646430578977|                 284640|\n",
      "|  Pennsylvania| 43.28163771712159|                 265980|\n",
      "|      Maryland| 43.82807308970099|                 240800|\n",
      "|     Minnesota| 42.35060896605338|                 208386|\n",
      "|     Louisiana| 41.91935799569387|                 204360|\n",
      "+--------------+------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# review the highest city and age rate & total Population when the reason of visa type for Business\n",
    "spark.sql(\"\"\"\n",
    "SELECT f.state, AVG(f.age),COUNT(d.total_population)\n",
    "FROM fact_immigration_table AS f\n",
    "JOIN dim_demographics_table AS d\n",
    "ON f.state = d.state\n",
    "WHERE visa_type = \"Business\"\n",
    "GROUP BY 1\n",
    "ORDER BY 3 DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descrition of Immigration Data:\n",
    "\n",
    "| Feature | Description |\n",
    "| ------- | ----------  |\n",
    "| cicid | Unique ID |\n",
    "| i94yr | Year |\n",
    "| i94mon | Month |\n",
    "| i94cit | 3 digit code for immigrant country of brith|\n",
    "| i94res | 3 digit code for immigrant country of residence |\n",
    "| i94port | Port of admission |\n",
    "| arrdate | Arrival Date in the USA |\n",
    "| i94mode | Mode of transportation (1 = Air, 2 = Sea, 3 = Land, 9 = Not reported) |\n",
    "| i94addr | USA State of arrival |\n",
    "| depdate | Departure DAte From the USA |\n",
    "| i94bir | Age of Respondent in Years |\n",
    "| i94visa | Visa codes collapsed into three categories |\n",
    "| count | Field used for summary statistics |\n",
    "| dtadfile | Character Date Field - DAte added to I-94 Files |\n",
    "| visapost | Department of State where Visa was issued |\n",
    "| occup | Occupation that will be performed is U.S |\n",
    "| entdepa | Arrival Flag - admitted or paroled into th US |\n",
    "| entdepd | Departure Flag - Departed, lost I-94 or is deceased |\n",
    "| entdepu | Update Flag - Either apperhended, Overstayed, adjusted to perm residence |\n",
    "| matflag | Match flag - Match of arrival and departure records |\n",
    "| biryear | 4 digit year of birth |\n",
    "| dtaddto | Character Date Field - Date to which admitted to U.S (allowed to stay until) |\n",
    "| gender | Non - immigrant sex |\n",
    "| insnum | INS number |\n",
    "| airline | Airline used to arrive in U.S. |\n",
    "| admnum | Admission Number |\n",
    "| fltno | Flight number od Airline used to arrive in U.S. |\n",
    "| visatype | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Temperature Data:\n",
    "\n",
    "| Feature | Description |\n",
    "| ------- | ----------- |\n",
    "| dt | Date |\n",
    "| AverageTemperature | Average temerature in celsius |\n",
    "| AvarageTemperatureUncertainty | 95% confidence interval around average temperature |\n",
    "| City | Name of city |\n",
    "| Country | Name of country |\n",
    "| Latitude | Latitude of city |\n",
    "| Longitude | Longitude of city |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Demographics Data:\n",
    "\n",
    "| Feature | Description |\n",
    "| ------- | ----------- |\n",
    "| City | City Name |\n",
    "| State | US State of City |\n",
    "| Median Age | The median population age |\n",
    "| Male Population | Male population total |\n",
    "| Female Population | Female population total |\n",
    "| Total Population | Total population |\n",
    "| Number of Veterans | Number of veterans living in the city |\n",
    "| Foregin- born | Number of residents who were not born in the city |\n",
    "| Average Household Size | Average size of houses in the city |\n",
    "| state Code | Code of the state |\n",
    "| Race | race class |\n",
    "| Count | Number of individuals in each race |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Airport Data\n",
    "\n",
    "| Feature | Description |\n",
    "| ------- | ----------- |\n",
    "| ident | Unique identifier |\n",
    "| type | Airport type |\n",
    "| name | Airport name |\n",
    "| elevation_ft | Airport altitude |\n",
    "| countinent | Continent |\n",
    "| iso_country | ISO Code of the airport's country |\n",
    "| iso_region | ISO Code for the airport's region |\n",
    "| municipality | City/Municipality where the airport is located |\n",
    "| gps_code | Airport GPS Code |\n",
    "| lata_code | Airport IATA Code |\n",
    "| local_code | Airport local Code |\n",
    "| coordinates | Airport coordinates |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "this project using a big data Apache Spark processing technologie because of ability to process massive amounts of data as well as the use of unified analytics engine and convenient API's.\n",
    "\n",
    "#### Requirment scenarios:\n",
    "\n",
    "1- The data was increased by 100x:\n",
    "\n",
    "In this scenario the data would be stored on one of storage and processing platforms such as Amazon S3 bucket or Amazon Redshift or Amazon EMR culster or Apache Cassandera and loaded into our staging tables for processing and analysis.\n",
    "\n",
    "2- The data population a dashboard that must be updated on a daily basis by 7am every day:\n",
    "\n",
    "In this scenario we would manage ETL pipeline in DAG from Apache Airflow to ensure the pipeline runs in set time and data quality check test as well.\n",
    "\n",
    "3- The datebase needed to be accessed by 100+ people:\n",
    "\n",
    "In this scenario better move the database to Amazon Redshift cluster which can handle a massive request volumes and it will be easy to scalable as require.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
